use crate::agent::{AgentConfig, AgentContent, AgentEvent, AgentLoop, AgentMessage};
use crate::agent::{ContentBlock, ImageSource};
use crate::claude::{ClaudeClient, Message as ClaudeMessage};
use crate::database::{Conversation, Database, Message, PlanStep, Settings, Task, TaskMessage};
use crate::mcp::{MCPManager, MCPServerConfig, MCPServerStatus, MCPToolCall, MCPToolResult};
use crate::skills::{SkillMetadata, get_available_skills};
use base64::{Engine as _, engine::general_purpose};
use regex::Regex;
use serde::{Deserialize, Serialize};
use std::fs;
use std::path::PathBuf;
use std::sync::Arc;
use tauri::{command, Emitter, State, Window};
use tokio::sync::Mutex;

pub struct AppState {
    pub db: Arc<Database>,
    pub claude_client: Mutex<Option<ClaudeClient>>,
    pub mcp_manager: Arc<MCPManager>,
}

#[derive(Debug, Serialize)]
pub struct CommandError {
    message: String,
}

fn default_workspace_root() -> Option<String> {
    std::env::current_dir()
        .ok()
        .map(|p| p.to_string_lossy().to_string())
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct LocalModelInfo {
    pub name: String,
    pub size: u64,
    pub modified_at: String,
    pub digest: String,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct LocalServiceStatus {
    pub running: bool,
    pub models: Vec<LocalModelInfo>,
    pub error: Option<String>,
}

impl From<crate::database::DbError> for CommandError {
    fn from(e: crate::database::DbError) -> Self {
        CommandError {
            message: e.to_string(),
        }
    }
}

impl From<crate::claude::ClaudeError> for CommandError {
    fn from(e: crate::claude::ClaudeError) -> Self {
        CommandError {
            message: e.to_string(),
        }
    }
}

// Platform command
#[command]
pub fn get_platform() -> String {
    #[cfg(target_os = "macos")]
    return "darwin".to_string();

    #[cfg(target_os = "windows")]
    return "windows".to_string();

    #[cfg(target_os = "linux")]
    return "linux".to_string();

    #[cfg(not(any(target_os = "macos", target_os = "windows", target_os = "linux")))]
    return "unknown".to_string();
}

// Settings commands
#[command]
pub fn get_settings(state: State<'_, Arc<AppState>>) -> Result<Settings, CommandError> {
    let settings = state.db.get_settings()?;
    println!("[get_settings] api_key length from db: {}", settings.api_key.len());
    Ok(settings)
}

#[command]
pub async fn save_settings(
    state: State<'_, Arc<AppState>>,
    settings: Settings,
) -> Result<(), CommandError> {
    println!("[save_settings] model: {}", settings.model);
    println!("[save_settings] base_url: {}", settings.base_url);
    println!("[save_settings] api_key length: {}", settings.api_key.len());
    // Show first and last 10 chars for debugging
    if settings.api_key.len() > 20 {
        println!("[save_settings] api_key preview: {}...{}",
            &settings.api_key[..10],
            &settings.api_key[settings.api_key.len()-10..]);
    }

    state.db.save_settings(&settings)?;

    // Update Claude client with new settings
    let mut client = state.claude_client.lock().await;
    if !settings.api_key.is_empty() {
        *client = Some(ClaudeClient::new(
            settings.api_key.clone(),
            Some(settings.base_url.clone()),
        ));
    } else {
        *client = None;
    }

    Ok(())
}

#[command]
pub async fn test_connection(state: State<'_, Arc<AppState>>) -> Result<String, CommandError> {
    use crate::llm_client::{LLMClient, Message};

    let settings = state.db.get_settings()?;

    // Debug logging
    println!("[test_connection] model: {}", settings.model);
    println!("[test_connection] base_url: {}", settings.base_url);
    println!("[test_connection] api_key length: {}", settings.api_key.len());
    println!("[test_connection] provider: {}", settings.get_provider());
    println!("[test_connection] is_local_provider: {}, allows_empty_api_key: {}",
        settings.is_local_provider(), settings.allows_empty_api_key());

    if settings.api_key.is_empty() && !settings.allows_empty_api_key() {
        return Ok("No API key configured".to_string());
    }

    // Choose test method based on provider type
    if settings.is_local_provider() {
        // Local service - use LLMClient to check connection
        let llm_client = LLMClient::new(
            String::new(), // Local services don't need API key
            Some(settings.base_url.clone()),
            None,
            Some(&settings.model),
        );

        match llm_client.check_connection().await {
            Ok(true) => Ok("success".to_string()),
            Ok(false) => Ok("Error: Cannot connect to local service, please ensure it is running".to_string()),
            Err(e) => Ok(format!("Error: {}", e)),
        }
    } else {
        // Cloud service - check provider type
        let provider = settings.get_provider();

        match provider.as_str() {
            "anthropic" => {
                // Anthropic - use ClaudeClient
                let client = ClaudeClient::new(settings.api_key, Some(settings.base_url));
                let messages = vec![ClaudeMessage {
                    role: "user".to_string(),
                    content: "Hi".to_string(),
                }];

                match client.send_message(messages, &settings.model, 10, None).await {
                    Ok(_) => Ok("success".to_string()),
                    Err(e) => Ok(format!("Error: {}", e)),
                }
            }
            "openai" => {
                // OpenAI - test with actual API request using LLMClient
                let llm_client = LLMClient::new_with_openai_headers(
                    settings.api_key.clone(),
                    Some(settings.base_url.clone()),
                    Some("openai"),
                    Some(&settings.model),
                    settings.openai_organization.clone(),
                    settings.openai_project.clone(),
                );

                let test_messages = vec![Message {
                    role: "user".to_string(),
                    content: "Hi".to_string(),
                }];

                // Send a minimal test request
                match llm_client.send_message(test_messages, &settings.model, 10, None).await {
                    Ok(_) => Ok("success".to_string()),
                    Err(e) => Ok(format!("Error: {}", e)),
                }
            }
            "google" => {
                // Google Gemini - test with actual API request
                let llm_client = LLMClient::new(
                    settings.api_key.clone(),
                    Some(settings.base_url.clone()),
                    Some("google"),
                    Some(&settings.model),
                );

                let test_messages = vec![Message {
                    role: "user".to_string(),
                    content: "Hi".to_string(),
                }];

                match llm_client.send_message(test_messages, &settings.model, 10, None).await {
                    Ok(_) => Ok("success".to_string()),
                    Err(e) => Ok(format!("Error: {}", e)),
                }
            }
            _ => {
                // Other cloud services - try sending a test message
                let llm_client = LLMClient::new(
                    settings.api_key.clone(),
                    Some(settings.base_url.clone()),
                    None,
                    Some(&settings.model),
                );

                let test_messages = vec![Message {
                    role: "user".to_string(),
                    content: "Hi".to_string(),
                }];

                // Try to send a minimal test request
                match llm_client.send_message(test_messages, &settings.model, 10, None).await {
                    Ok(_) => Ok("success".to_string()),
                    Err(e) => {
                        // If sending fails, try simple connection check (for services that support it)
                        match llm_client.check_connection().await {
                            Ok(true) => Ok("success".to_string()),
                            Ok(false) => Ok(format!("Error: {}", e)),
                            Err(conn_e) => Ok(format!("Error: {}", conn_e)),
                        }
                    }
                }
            }
        }
    }
}

#[command]
pub async fn check_local_service_status(base_url: String) -> LocalServiceStatus {
    let client = reqwest::Client::new();
    let base = base_url.trim_end_matches('/');
    let normalized = base.strip_suffix("/v1").unwrap_or(base);
    let url = format!("{}/api/tags", normalized);

    let response = match client
        .get(&url)
        .timeout(std::time::Duration::from_secs(5))
        .send()
        .await
    {
        Ok(resp) => resp,
        Err(e) => {
            return LocalServiceStatus {
                running: false,
                models: vec![],
                error: Some(e.to_string()),
            }
        }
    };

    if !response.status().is_success() {
        return LocalServiceStatus {
            running: false,
            models: vec![],
            error: Some(format!("HTTP {}", response.status())),
        };
    }

    let data: serde_json::Value = match response.json().await {
        Ok(json) => json,
        Err(e) => {
            return LocalServiceStatus {
                running: false,
                models: vec![],
                error: Some(e.to_string()),
            }
        }
    };

    let models = data["models"]
        .as_array()
        .map(|arr| {
            arr.iter()
                .map(|m| LocalModelInfo {
                    name: m["name"].as_str().unwrap_or("").to_string(),
                    size: m["size"].as_u64().unwrap_or(0),
                    modified_at: m["modified_at"].as_str().unwrap_or("").to_string(),
                    digest: m["digest"].as_str().unwrap_or("").to_string(),
                })
                .collect::<Vec<_>>()
        })
        .unwrap_or_default();

    LocalServiceStatus {
        running: true,
        models,
        error: None,
    }
}

// Conversation commands
#[command]
pub fn list_conversations(
    state: State<'_, Arc<AppState>>,
) -> Result<Vec<Conversation>, CommandError> {
    state.db.list_conversations().map_err(Into::into)
}

#[command]
pub fn create_conversation(
    state: State<'_, Arc<AppState>>,
    title: String,
) -> Result<Conversation, CommandError> {
    let id = uuid::Uuid::new_v4().to_string();
    state.db.create_conversation(&id, &title).map_err(Into::into)
}

#[command]
pub fn update_conversation_title(
    state: State<'_, Arc<AppState>>,
    id: String,
    title: String,
) -> Result<(), CommandError> {
    state.db.update_conversation_title(&id, &title).map_err(Into::into)
}

#[command]
pub fn delete_conversation(
    state: State<'_, Arc<AppState>>,
    id: String,
) -> Result<(), CommandError> {
    state.db.delete_conversation(&id).map_err(Into::into)
}

// Message commands
#[command]
pub fn get_messages(
    state: State<'_, Arc<AppState>>,
    conversation_id: String,
) -> Result<Vec<Message>, CommandError> {
    state.db.get_messages(&conversation_id).map_err(Into::into)
}

#[command]
pub fn add_message(
    state: State<'_, Arc<AppState>>,
    conversation_id: String,
    role: String,
    content: String,
) -> Result<Message, CommandError> {
    let id = uuid::Uuid::new_v4().to_string();
    state
        .db
        .add_message(&id, &conversation_id, &role, &content)
        .map_err(Into::into)
}

// Chat command with streaming
#[derive(Clone, Serialize)]
struct StreamPayload {
    text: String,
    done: bool,
}

#[command]
pub async fn send_chat_message(
    window: Window,
    state: State<'_, Arc<AppState>>,
    conversation_id: String,
    content: String,
) -> Result<String, CommandError> {
    use crate::llm_client::{LLMClient, Message as LLMMessage};

    let settings = state.db.get_settings()?;

    if settings.api_key.is_empty() && !settings.allows_empty_api_key() {
        return Err(CommandError {
            message: "API key not configured".to_string(),
        });
    }

    // Add user message to database
    let user_msg_id = uuid::Uuid::new_v4().to_string();
    state
        .db
        .add_message(&user_msg_id, &conversation_id, "user", &content)?;

    // Get conversation history
    let db_messages = state.db.get_messages(&conversation_id)?;

    // Create channel for streaming
    let (tx, mut rx) = tokio::sync::mpsc::channel::<String>(100);

    // Spawn task to emit events
    let window_clone = window.clone();
    let emit_task = tokio::spawn(async move {
        while let Some(text) = rx.recv().await {
            let _ = window_clone.emit("chat-stream", StreamPayload { text, done: false });
        }
    });

    // Choose client based on provider
    let provider = settings.get_provider();
    let response = match provider.as_str() {
        "anthropic" => {
            // Use ClaudeClient for Anthropic
            let claude_messages: Vec<ClaudeMessage> = db_messages
                .iter()
                .map(|m| ClaudeMessage {
                    role: m.role.clone(),
                    content: m.content.clone(),
                })
                .collect();
            let client = ClaudeClient::new(settings.api_key, Some(settings.base_url));
            client
                .send_message_stream(
                    claude_messages,
                    &settings.model,
                    settings.max_tokens,
                    Some(settings.temperature),
                    tx,
                )
                .await?
        }
        _ => {
            // Use LLMClient for OpenAI and other providers
            let llm_messages: Vec<LLMMessage> = db_messages
                .iter()
                .map(|m| LLMMessage {
                    role: m.role.clone(),
                    content: m.content.clone(),
                })
                .collect();
            let llm_client = LLMClient::new_with_openai_headers(
                settings.api_key.clone(),
                Some(settings.base_url.clone()),
                Some(&provider),
                Some(&settings.model),
                settings.openai_organization.clone(),
                settings.openai_project.clone(),
            );
            llm_client
                .send_message_stream(
                    llm_messages,
                    &settings.model,
                    settings.max_tokens,
                    Some(settings.temperature),
                    tx,
                )
                .await
                .map_err(|e| CommandError { message: e.to_string() })?
        }
    };

    // Wait for emit task to finish
    let _ = emit_task.await;

    // Emit done event
    let _ = window.emit(
        "chat-stream",
        StreamPayload {
            text: response.clone(),
            done: true,
        },
    );

    // Save assistant response to database
    let assistant_msg_id = uuid::Uuid::new_v4().to_string();
    state
        .db
        .add_message(&assistant_msg_id, &conversation_id, "assistant", &response)?;

    // Update conversation title if this is the first message
    if db_messages.len() == 1 {
        let title = if content.len() > 30 {
            format!("{}...", &content[..30])
        } else {
            content.clone()
        };
        state.db.update_conversation_title(&conversation_id, &title)?;
    }

    Ok(response)
}

// Chat event for tool-enabled chat
#[derive(Debug, Clone, Serialize)]
#[serde(tag = "type")]
pub enum ChatEvent {
    #[serde(rename = "text")]
    Text { content: String },
    #[serde(rename = "tool_start")]
    ToolStart { tool: String, input: serde_json::Value },
    #[serde(rename = "tool_end")]
    ToolEnd { tool: String, result: String, success: bool },
    #[serde(rename = "done")]
    Done { final_text: String },
}

#[derive(Debug, Clone)]
struct ForcedToolPreview {
    tool: String,
    input: serde_json::Value,
    result: String,
    success: bool,
}

#[derive(Debug, Clone)]
struct ForcedExecution {
    final_text: String,
    previews: Vec<ForcedToolPreview>,
}

// Agent command
#[derive(Debug, Deserialize)]
pub struct AgentRequest {
    pub message: String,
    pub project_path: Option<String>,
    pub system_prompt: Option<String>,
    pub max_turns: Option<u32>,
}

#[command]
pub async fn run_agent(
    window: Window,
    state: State<'_, Arc<AppState>>,
    request: AgentRequest,
) -> Result<String, CommandError> {
    let settings = state.db.get_settings()?;

    // Check if API Key is needed (local services don't need it)
    if settings.api_key.is_empty() && !settings.allows_empty_api_key() {
        return Err(CommandError {
            message: "API key not configured".to_string(),
        });
    }

    // Build agent config
    let mut config = AgentConfig::default();
    if let Some(prompt) = request.system_prompt {
        config.system_prompt = prompt;
    } else {
        // Add MCP servers info to default system prompt
        let mcp_servers = state.mcp_manager.get_server_statuses().await;
        let mut mcp_info = String::new();
        if !mcp_servers.is_empty() {
            mcp_info.push_str("\nMCP (Model Context Protocol) Tools:\n");
            for server in mcp_servers {
                if matches!(server.status, crate::mcp::types::ConnectionStatus::Connected) {
                    mcp_info.push_str(&format!("Server '{}' is connected with tools:\n", server.id));
                    for tool in server.tools {
                        mcp_info.push_str(&format!("  - {}: {} (use format: {}:{})\n",
                            tool.name, tool.description, server.id, tool.name));
                    }
                }
            }
        }
        if !mcp_info.is_empty() {
            config.system_prompt.push_str(&mcp_info);
        }
    }
    if let Some(turns) = request.max_turns {
        config.max_turns = turns;
    }
    config.project_path = request.project_path.or_else(default_workspace_root);

    // Get provider info
    let provider_id = settings.get_provider();

    // Create agent loop with provider
    let agent = AgentLoop::new_with_provider(
        settings.api_key,
        settings.base_url,
        config,
        settings.model,
        settings.max_tokens,
        Some(settings.temperature),
        state.mcp_manager.clone(),
        Some(&provider_id),
    );

    // Create channel for events
    let (tx, mut rx) = tokio::sync::mpsc::channel::<AgentEvent>(100);

    // Spawn event emitter
    let window_clone = window.clone();
    let emit_task = tokio::spawn(async move {
        while let Some(event) = rx.recv().await {
            let _ = window_clone.emit("agent-event", &event);
        }
    });

    // Run agent
    let result = agent.run(request.message, tx).await;

    // Wait for emitter to finish
    let _ = emit_task.await;

    match result {
        Ok(_messages) => Ok("Agent completed successfully".to_string()),
        Err(e) => Err(CommandError { message: e }),
    }
}

// Enhanced chat with tools - integrates agent capabilities into chat
#[derive(Debug, Deserialize)]
pub struct EnhancedChatRequest {
    pub conversation_id: String,
    pub content: String,
    pub project_path: Option<String>,
    pub enable_tools: bool,
}

#[command]
pub async fn send_chat_with_tools(
    window: Window,
    state: State<'_, Arc<AppState>>,
    request: EnhancedChatRequest,
) -> Result<String, CommandError> {
    use crate::agent::{
        AgentConfig, AgentContent, AgentMessage, ContentBlock, MessageBuilder, ToolExecutor, ToolUse,
    };
    use futures::StreamExt;

    let settings = state.db.get_settings()?;

    if settings.api_key.is_empty() && !settings.allows_empty_api_key() {
        return Err(CommandError {
            message: "API key not configured".to_string(),
        });
    }

    // Add user message to database
    let user_msg_id = uuid::Uuid::new_v4().to_string();
    state
        .db
        .add_message(&user_msg_id, &request.conversation_id, "user", &request.content)?;

    // Get conversation history
    let db_messages = state.db.get_messages(&request.conversation_id)?;

    // If tools are not enabled, fall back to simple chat
    if !request.enable_tools {
        use crate::llm_client::{LLMClient, Message as LLMMessage};

        let provider = settings.get_provider();
        let (tx, mut rx) = tokio::sync::mpsc::channel::<String>(100);

        let window_clone = window.clone();
        let emit_task = tokio::spawn(async move {
            while let Some(text) = rx.recv().await {
                let _ = window_clone.emit("chat-event", ChatEvent::Text { content: text });
            }
        });

        let response = match provider.as_str() {
            "anthropic" => {
                // Use ClaudeClient for Anthropic
                let claude_messages: Vec<ClaudeMessage> = db_messages
                    .iter()
                    .map(|m| ClaudeMessage {
                        role: m.role.clone(),
                        content: m.content.clone(),
                    })
                    .collect();
                let client = ClaudeClient::new(settings.api_key.clone(), Some(settings.base_url.clone()));
                client
                    .send_message_stream(
                        claude_messages,
                        &settings.model,
                        settings.max_tokens,
                        Some(settings.temperature),
                        tx,
                    )
                    .await?
            }
            _ => {
                // Use LLMClient for OpenAI and other providers
                let llm_messages: Vec<LLMMessage> = db_messages
                    .iter()
                    .map(|m| LLMMessage {
                        role: m.role.clone(),
                        content: m.content.clone(),
                    })
                    .collect();
                let llm_client = LLMClient::new_with_openai_headers(
                    settings.api_key.clone(),
                    Some(settings.base_url.clone()),
                    Some(&provider),
                    Some(&settings.model),
                    settings.openai_organization.clone(),
                    settings.openai_project.clone(),
                );
                llm_client
                    .send_message_stream(
                        llm_messages,
                        &settings.model,
                        settings.max_tokens,
                        Some(settings.temperature),
                        tx,
                    )
                    .await
                    .map_err(|e| CommandError { message: e.to_string() })?
            }
        };

        let _ = emit_task.await;
        let _ = window.emit("chat-event", ChatEvent::Done { final_text: response.clone() });

        // Save assistant response
        let assistant_msg_id = uuid::Uuid::new_v4().to_string();
        state
            .db
            .add_message(&assistant_msg_id, &request.conversation_id, "assistant", &response)?;

        return Ok(response);
    }

    // Enhanced chat with tools - use AgentLoop which supports multiple providers
    use crate::llm_client::ProviderConfig;

    let effective_project_path = request.project_path.clone().or_else(default_workspace_root);

    let tool_executor = ToolExecutor::new(effective_project_path.clone())
        .with_mcp_manager(state.mcp_manager.clone());

    // Build agent-style config for tools
    let mut config = AgentConfig {
        project_path: effective_project_path.clone(),
        max_turns: 10, // Limit turns in chat mode
        ..Default::default()
    };

    // System prompt for chat with tools - include MCP servers info
    let mcp_servers = state.mcp_manager.get_server_statuses().await;
    let mut mcp_info = String::new();
    if !mcp_servers.is_empty() {
        mcp_info.push_str("\nMCP (Model Context Protocol) Tools:\n");
        for server in mcp_servers {
            if matches!(server.status, crate::mcp::types::ConnectionStatus::Connected) {
                mcp_info.push_str(&format!("Server '{}' is connected with tools:\n", server.id));
                for tool in server.tools {
                    mcp_info.push_str(&format!("  - {}: {} (use format: {}:{})\n",
                        tool.name, tool.description, server.id, tool.name));
                }
            }
        }
    }

    config.system_prompt = format!(r#"You are Kuse Cowork, an AI assistant that helps users for non dev work.

You have access to tools that allow you to read and write files, execute commands, and search through codebases.

When the user asks you to do something that requires accessing files or running commands, use the appropriate tools.
For simple questions or conversations, respond directly without using tools.

Be concise and helpful. Explain what you're doing when using tools.{}"#, mcp_info);
    config.system_prompt.push_str(
        "\n\n## Response Quality Rules\n\
        - After any tool call, respond only with results grounded in that tool output.\n\
        - Do not add unrelated explanations about project structure or technology stacks.\n\
        - If the user explicitly asks to use a specific tool, execute it and return a short outcome-focused response.\n"
    );
    if let Some(project_path) = &effective_project_path {
        config.system_prompt.push_str(&format!(
            "\n\n## Workspace Constraints\nMounted folder(s): {}\nAlways read and write files only inside mounted folder(s). Avoid temporary directories unless user explicitly asks.",
            project_path
        ));
    }

    if let Some(forced) = try_force_xlsx_creation(&request.content, effective_project_path.as_deref()) {
        for preview in &forced.previews {
            let _ = window.emit("chat-event", ChatEvent::ToolStart {
                tool: preview.tool.clone(),
                input: preview.input.clone(),
            });
            let _ = window.emit("chat-event", ChatEvent::ToolEnd {
                tool: preview.tool.clone(),
                result: preview.result.clone(),
                success: preview.success,
            });
        }
        let _ = window.emit("chat-event", ChatEvent::Text { content: forced.final_text.clone() });
        let _ = window.emit("chat-event", ChatEvent::Done { final_text: forced.final_text.clone() });
        let assistant_msg_id = uuid::Uuid::new_v4().to_string();
        state
            .db
            .add_message(&assistant_msg_id, &request.conversation_id, "assistant", &forced.final_text)?;
        return Ok(forced.final_text);
    }

    if let Some(forced) = try_force_directory_listing(&state.mcp_manager, &request.content).await {
        for preview in &forced.previews {
            let _ = window.emit("chat-event", ChatEvent::ToolStart {
                tool: preview.tool.clone(),
                input: preview.input.clone(),
            });
            let _ = window.emit("chat-event", ChatEvent::ToolEnd {
                tool: preview.tool.clone(),
                result: preview.result.clone(),
                success: preview.success,
            });
        }
        let _ = window.emit("chat-event", ChatEvent::Text { content: forced.final_text.clone() });
        let _ = window.emit("chat-event", ChatEvent::Done { final_text: forced.final_text.clone() });
        let assistant_msg_id = uuid::Uuid::new_v4().to_string();
        state
            .db
            .add_message(&assistant_msg_id, &request.conversation_id, "assistant", &forced.final_text)?;
        return Ok(forced.final_text);
    }

    let message_builder = MessageBuilder::new(
        config.clone(),
        settings.model.clone(),
        settings.max_tokens,
        Some(settings.temperature),
    );

    // Convert DB messages to agent messages
    let mut agent_messages: Vec<AgentMessage> = db_messages
        .iter()
        .map(|m| AgentMessage {
            role: m.role.clone(),
            content: AgentContent::Text(m.content.clone()),
        })
        .collect();

    let client = reqwest::Client::new();
    let mut final_text = String::new();
    let mut last_tool_output: Option<String> = None;
    let mut tool_call_count: usize = 0;
    let mut turn = 0;
    let max_turns = config.max_turns;

    // Get provider config for determining API format
    let provider_id = settings.get_provider();
    let mut provider_config = ProviderConfig::from_preset(&provider_id);
    if !settings.base_url.is_empty() {
        provider_config.base_url = settings.base_url.clone();
    }

    // Determine API format
    let use_openai_format = matches!(
        provider_config.api_format,
        crate::llm_client::ApiFormat::OpenAI | crate::llm_client::ApiFormat::OpenAICompatible
    );
    let use_google_format = matches!(
        provider_config.api_format,
        crate::llm_client::ApiFormat::Google
    );

    // For Google: track thoughtSignature per function call across iterations (required for Gemini 3)
    let mut google_thought_signatures: std::collections::HashMap<String, String> = std::collections::HashMap::new();

    loop {
        turn += 1;
        if turn > max_turns {
            break;
        }

        // Build and send request
        let api_request = message_builder.build_request(&agent_messages).await;

        let response = if use_google_format {
            // Google Gemini format request (pass thought signatures for Gemini 3 function calling)
            let google_request = convert_to_google_format(&api_request, &settings.model, settings.max_tokens, &google_thought_signatures);
            let base = provider_config.base_url.trim_end_matches('/');
            let url = format!("{}/v1beta/models/{}:streamGenerateContent?alt=sse", base, settings.model);

            client.post(&url)
                .header("Content-Type", "application/json")
                .header("x-goog-api-key", &settings.api_key)
                .json(&google_request)
                .send()
                .await
                .map_err(|e| CommandError { message: format!("HTTP error: {}", e) })?
        } else if use_openai_format {
            // OpenAI format request
            let openai_request = convert_to_openai_format(&api_request, &settings.model);
            let base = provider_config.base_url.trim_end_matches('/');
            let url = if base.ends_with("/v1") {
                format!("{}/chat/completions", base)
            } else {
                format!("{}/v1/chat/completions", base)
            };

            let mut req = client.post(&url)
                .header("Content-Type", "application/json");

            if !settings.api_key.is_empty() {
                req = req.header("Authorization", format!("Bearer {}", settings.api_key));
            }
            // Add optional OpenAI headers
            if let Some(ref org) = settings.openai_organization {
                if !org.is_empty() {
                    req = req.header("OpenAI-Organization", org);
                }
            }
            if let Some(ref proj) = settings.openai_project {
                if !proj.is_empty() {
                    req = req.header("OpenAI-Project", proj);
                }
            }

            req.json(&openai_request)
                .send()
                .await
                .map_err(|e| CommandError { message: format!("HTTP error: {}", e) })?
        } else {
            // Anthropic format request
            client
                .post(format!("{}/v1/messages", provider_config.base_url.trim_end_matches('/')))
                .header("Content-Type", "application/json")
                .header("x-api-key", &settings.api_key)
                .header("anthropic-version", "2023-06-01")
                .json(&api_request)
                .send()
                .await
                .map_err(|e| CommandError { message: format!("HTTP error: {}", e) })?
        };

        if !response.status().is_success() {
            let error_text = response.text().await.unwrap_or_default();
            return Err(CommandError { message: format!("API error: {}", error_text) });
        }

        // Handle streaming response based on provider format
        let mut stream = response.bytes_stream();
        let mut buffer = String::new();
        let mut accumulated_text = String::new();
        let mut tool_uses: Vec<ToolUse> = Vec::new();

        if use_google_format {
            // Google Gemini streaming format (SSE with alt=sse)
            while let Some(chunk) = stream.next().await {
                let chunk = chunk.map_err(|e| CommandError { message: format!("Stream error: {}", e) })?;
                buffer.push_str(&String::from_utf8_lossy(&chunk));

                while let Some(pos) = buffer.find('\n') {
                    let line = buffer[..pos].trim().to_string();
                    buffer = buffer[pos + 1..].to_string();

                    if line.is_empty() {
                        continue;
                    }

                    // Parse SSE data: prefix
                    let json_str = if let Some(data) = line.strip_prefix("data: ") {
                        data
                    } else {
                        continue;
                    };

                    if let Ok(event) = serde_json::from_str::<serde_json::Value>(json_str) {
                        // Extract text and function calls from candidates
                        if let Some(candidates) = event.get("candidates").and_then(|v| v.as_array()) {
                            for candidate in candidates {
                                if let Some(parts) = candidate.get("content")
                                    .and_then(|c| c.get("parts"))
                                    .and_then(|p| p.as_array())
                                {
                                    for part in parts {
                                        // Handle text
                                        if let Some(text) = part.get("text").and_then(|v| v.as_str()) {
                                            if !text.is_empty() {
                                                accumulated_text.push_str(text);
                                                let _ = window.emit("chat-event", ChatEvent::Text {
                                                    content: accumulated_text.clone(),
                                                });
                                            }
                                        }
                                        // Handle function calls (with thoughtSignature for Gemini 3)
                                        if let Some(fc) = part.get("functionCall") {
                                            let name = fc.get("name").and_then(|v| v.as_str()).unwrap_or("").to_string();
                                            let args = fc.get("args").cloned().unwrap_or(serde_json::json!({}));
                                            let id = format!("fc_{}", uuid::Uuid::new_v4());

                                            // Capture thoughtSignature from the same part (required for Gemini 3)
                                            let thought_signature = part.get("thoughtSignature")
                                                .and_then(|v| v.as_str())
                                                .map(|s| s.to_string());

                                            // Also store in map for lookup when building functionResponse
                                            if let Some(ref sig) = thought_signature {
                                                google_thought_signatures.insert(id.clone(), sig.clone());
                                            }

                                            tool_uses.push(ToolUse {
                                                id: id.clone(),
                                                name: name.clone(),
                                                input: args.clone(),
                                                thought_signature,
                                            });

                                            let _ = window.emit("chat-event", ChatEvent::ToolStart {
                                                tool: name,
                                                input: args,
                                            });
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            }
        } else if use_openai_format {
            // OpenAI streaming format
            let mut current_tool_calls: std::collections::HashMap<i64, (String, String, String)> = std::collections::HashMap::new();

            while let Some(chunk) = stream.next().await {
                let chunk = chunk.map_err(|e| CommandError { message: format!("Stream error: {}", e) })?;
                buffer.push_str(&String::from_utf8_lossy(&chunk));

                while let Some(pos) = buffer.find('\n') {
                    let line = buffer[..pos].to_string();
                    buffer = buffer[pos + 1..].to_string();

                    if let Some(data) = line.strip_prefix("data: ") {
                        if data.trim() == "[DONE]" {
                            continue;
                        }

                        if let Ok(event) = serde_json::from_str::<serde_json::Value>(data) {
                            if let Some(choices) = event.get("choices").and_then(|v| v.as_array()) {
                                for choice in choices {
                                    if let Some(delta) = choice.get("delta") {
                                        // Handle text content
                                        if let Some(content) = delta.get("content").and_then(|v| v.as_str()) {
                                            accumulated_text.push_str(content);
                                            let _ = window.emit("chat-event", ChatEvent::Text {
                                                content: accumulated_text.clone(),
                                            });
                                        }

                                        // Handle tool_calls
                                        if let Some(tcs) = delta.get("tool_calls").and_then(|v| v.as_array()) {
                                            for tc in tcs {
                                                let index = tc.get("index").and_then(|v| v.as_i64()).unwrap_or(0);

                                                let entry = current_tool_calls.entry(index).or_insert_with(|| {
                                                    (String::new(), String::new(), String::new())
                                                });

                                                if let Some(id) = tc.get("id").and_then(|v| v.as_str()) {
                                                    entry.0 = id.to_string();
                                                }
                                                if let Some(func) = tc.get("function") {
                                                    if let Some(name) = func.get("name").and_then(|v| v.as_str()) {
                                                        entry.1 = name.to_string();
                                                    }
                                                    if let Some(args) = func.get("arguments").and_then(|v| v.as_str()) {
                                                        entry.2.push_str(args);
                                                    }
                                                }
                                            }
                                        }
                                    }

                                    // Check if finished
                                    if choice.get("finish_reason").and_then(|v| v.as_str()).is_some() {
                                        // Convert collected tool_calls to ToolUse
                                        for (id, name, args) in current_tool_calls.values() {
                                            if !id.is_empty() && !name.is_empty() {
                                                let input: serde_json::Value = serde_json::from_str(args)
                                                    .unwrap_or(serde_json::json!({}));

                                                tool_uses.push(ToolUse {
                                                    id: id.clone(),
                                                    name: name.clone(),
                                                    input: input.clone(),
                                                    thought_signature: None, // OpenAI doesn't use thought signatures
                                                });

                                                // Emit tool start
                                                let _ = window.emit("chat-event", ChatEvent::ToolStart {
                                                    tool: name.clone(),
                                                    input,
                                                });
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            }
        } else {
            // Anthropic streaming format
            let mut current_tool_input = String::new();
            let mut current_tool_id = String::new();
            let mut current_tool_name = String::new();

            while let Some(chunk) = stream.next().await {
                let chunk = chunk.map_err(|e| CommandError { message: format!("Stream error: {}", e) })?;
                buffer.push_str(&String::from_utf8_lossy(&chunk));

                while let Some(pos) = buffer.find('\n') {
                    let line = buffer[..pos].to_string();
                    buffer = buffer[pos + 1..].to_string();

                    if let Some(data) = line.strip_prefix("data: ") {
                        if data == "[DONE]" {
                            continue;
                        }

                        if let Ok(event) = serde_json::from_str::<serde_json::Value>(data) {
                            let event_type = event.get("type").and_then(|v| v.as_str()).unwrap_or("");

                            match event_type {
                                "content_block_start" => {
                                    if let Some(block) = event.get("content_block") {
                                        if block.get("type").and_then(|v| v.as_str()) == Some("tool_use") {
                                            current_tool_id = block
                                                .get("id")
                                                .and_then(|v| v.as_str())
                                                .unwrap_or("")
                                                .to_string();
                                            current_tool_name = block
                                                .get("name")
                                                .and_then(|v| v.as_str())
                                                .unwrap_or("")
                                                .to_string();
                                            current_tool_input.clear();
                                        }
                                    }
                                }
                                "content_block_delta" => {
                                    if let Some(delta) = event.get("delta") {
                                        let delta_type = delta.get("type").and_then(|v| v.as_str()).unwrap_or("");

                                        if delta_type == "text_delta" {
                                            if let Some(text) = delta.get("text").and_then(|v| v.as_str()) {
                                                accumulated_text.push_str(text);
                                                let _ = window.emit("chat-event", ChatEvent::Text {
                                                    content: accumulated_text.clone(),
                                                });
                                            }
                                        } else if delta_type == "input_json_delta" {
                                            if let Some(partial) = delta.get("partial_json").and_then(|v| v.as_str()) {
                                                current_tool_input.push_str(partial);
                                            }
                                        }
                                    }
                                }
                                "content_block_stop" => {
                                    if !current_tool_id.is_empty() {
                                        let input: serde_json::Value = serde_json::from_str(&current_tool_input)
                                            .unwrap_or(serde_json::json!({}));

                                        tool_uses.push(ToolUse {
                                            id: current_tool_id.clone(),
                                            name: current_tool_name.clone(),
                                            input: input.clone(),
                                            thought_signature: None, // Anthropic doesn't use thought signatures
                                        });

                                        // Emit tool start
                                        let _ = window.emit("chat-event", ChatEvent::ToolStart {
                                            tool: current_tool_name.clone(),
                                            input,
                                        });

                                        current_tool_id.clear();
                                        current_tool_name.clear();
                                        current_tool_input.clear();
                                    }
                                }
                                _ => {}
                            }
                        }
                    }
                }
            }
        }

        // Update final text
        if !accumulated_text.is_empty() {
            final_text = accumulated_text.clone();
        }

        // Add assistant message to history
        let assistant_content = if tool_uses.is_empty() {
            AgentContent::Text(accumulated_text)
        } else {
            let mut blocks = Vec::new();
            if !accumulated_text.is_empty() {
                blocks.push(ContentBlock::Text { text: accumulated_text });
            }
            for tu in &tool_uses {
                blocks.push(ContentBlock::ToolUse {
                    id: tu.id.clone(),
                    name: tu.name.clone(),
                    input: tu.input.clone(),
                    thought_signature: tu.thought_signature.clone(),
                });
            }
            AgentContent::Blocks(blocks)
        };

        agent_messages.push(AgentMessage {
            role: "assistant".to_string(),
            content: assistant_content,
        });

        // If no tool uses, we're done
        if tool_uses.is_empty() {
            break;
        }

        // Execute tools
        let mut tool_results = Vec::new();

        for tool_use in &tool_uses {
            let result = tool_executor.execute(tool_use).await;
            tool_call_count += 1;
            if !result.content.trim().is_empty() {
                let mut summary = result.content.trim().to_string();
                if summary.chars().count() > 1200 {
                    summary = summary.chars().take(1200).collect::<String>() + "...";
                }
                last_tool_output = Some(summary);
            }

            // Emit tool end
            let _ = window.emit("chat-event", ChatEvent::ToolEnd {
                tool: tool_use.name.clone(),
                result: result.content.clone(),
                success: result.is_error.is_none(),
            });

            tool_results.push(result);
        }

        // Add tool results as user message
        agent_messages.push(AgentMessage {
            role: "user".to_string(),
            content: AgentContent::ToolResults(tool_results),
        });
    }

    if final_text.trim().is_empty() {
        final_text = if let Some(tool_output) = last_tool_output {
            format!(
                "I completed the tool execution but did not receive a final text response from the model.\n\nTool result summary:\n{}",
                tool_output
            )
        } else if tool_call_count > 0 {
            format!(
                "I executed {} tool call(s), but the model returned an empty final response.",
                tool_call_count
            )
        } else {
            "The model returned an empty response for this request.".to_string()
        };
    }

    // Emit done
    let _ = window.emit("chat-event", ChatEvent::Done { final_text: final_text.clone() });

    // Save final assistant response to database
    let assistant_msg_id = uuid::Uuid::new_v4().to_string();
    state
        .db
        .add_message(&assistant_msg_id, &request.conversation_id, "assistant", &final_text)?;

    // Update conversation title if this is the first exchange
    if db_messages.len() == 1 {
        let title = if request.content.len() > 30 {
            format!("{}...", &request.content[..30])
        } else {
            request.content.clone()
        };
        state.db.update_conversation_title(&request.conversation_id, &title)?;
    }

    Ok(final_text)
}

// Task commands
#[command]
pub fn list_tasks(state: State<'_, Arc<AppState>>) -> Result<Vec<Task>, CommandError> {
    state.db.list_tasks().map_err(Into::into)
}

#[command]
pub fn get_task(state: State<'_, Arc<AppState>>, id: String) -> Result<Option<Task>, CommandError> {
    state.db.get_task(&id).map_err(Into::into)
}

#[command]
pub fn create_task(
    state: State<'_, Arc<AppState>>,
    title: String,
    description: String,
    project_path: Option<String>,
) -> Result<Task, CommandError> {
    let id = uuid::Uuid::new_v4().to_string();
    state.db.create_task(&id, &title, &description, project_path.as_deref()).map_err(Into::into)
}

#[command]
pub fn delete_task(state: State<'_, Arc<AppState>>, id: String) -> Result<(), CommandError> {
    state.db.delete_task(&id).map_err(Into::into)
}

// Run agent with task tracking
#[derive(Debug, Deserialize)]
pub struct TaskAgentRequest {
    pub task_id: String,
    pub message: String,
    pub project_path: Option<String>,
    pub image_paths: Option<Vec<String>>,
    pub image_data: Option<Vec<ImageAttachmentInput>>,
    pub max_turns: Option<u32>,
}

#[derive(Debug, Deserialize, Clone)]
pub struct ImageAttachmentInput {
    pub name: Option<String>,
    pub media_type: String,
    pub data: String,
}

#[command]
pub async fn run_task_agent(
    window: Window,
    state: State<'_, Arc<AppState>>,
    request: TaskAgentRequest,
) -> Result<String, CommandError> {
    let settings = state.db.get_settings()?;
    let task = state.db.get_task(&request.task_id)?;
    let effective_project_path = request
        .project_path
        .clone()
        .or_else(|| task.as_ref().and_then(|t| t.project_path.clone()))
        .or_else(default_workspace_root);

    // Check if API Key is needed (local services don't need it)
    if settings.api_key.is_empty() && !settings.allows_empty_api_key() {
        return Err(CommandError {
            message: "API key not configured".to_string(),
        });
    }

    // Load existing conversation history
    let existing_messages = state.db.get_task_messages(&request.task_id)?;

    // Save new user message
    let user_msg_id = uuid::Uuid::new_v4().to_string();
    let mut attached_names: Vec<String> = request
        .image_paths
        .as_ref()
        .into_iter()
        .flatten()
        .filter_map(|p| std::path::Path::new(p).file_name().map(|s| s.to_string_lossy().to_string()))
        .collect();
    if let Some(inline_images) = &request.image_data {
        for (idx, img) in inline_images.iter().enumerate() {
            attached_names.push(
                img.name
                    .clone()
                    .unwrap_or_else(|| format!("pasted-image-{}", idx + 1)),
            );
        }
    }
    let user_text_for_db = if attached_names.is_empty() {
        request.message.clone()
    } else {
        format!("{}\n\n[Attached images: {}]", request.message, attached_names.join(", "))
    };
    state.db.add_task_message(&user_msg_id, &request.task_id, "user", &user_text_for_db)?;

    // Update task status to running
    state.db.update_task_status(&request.task_id, "running")?;

    if let Some(forced) = try_force_xlsx_creation(&request.message, effective_project_path.as_deref()) {
        for preview in &forced.previews {
            let _ = window.emit("agent-event", AgentEvent::ToolStart {
                tool: preview.tool.clone(),
                input: preview.input.clone(),
            });
            let _ = window.emit("agent-event", AgentEvent::ToolEnd {
                tool: preview.tool.clone(),
                result: preview.result.clone(),
                success: preview.success,
            });
        }
        let assistant_msg_id = uuid::Uuid::new_v4().to_string();
        let _ = state.db.add_task_message(&assistant_msg_id, &request.task_id, "assistant", &forced.final_text);
        let _ = state.db.update_task_status(&request.task_id, "completed");
        let _ = window.emit("agent-event", AgentEvent::Text { content: forced.final_text });
        let _ = window.emit("agent-event", AgentEvent::Done { total_turns: 1 });
        return Ok("Task completed successfully".to_string());
    }

    if let Some(forced) = try_force_directory_listing(&state.mcp_manager, &request.message).await {
        for preview in &forced.previews {
            let _ = window.emit("agent-event", AgentEvent::ToolStart {
                tool: preview.tool.clone(),
                input: preview.input.clone(),
            });
            let _ = window.emit("agent-event", AgentEvent::ToolEnd {
                tool: preview.tool.clone(),
                result: preview.result.clone(),
                success: preview.success,
            });
        }
        let assistant_msg_id = uuid::Uuid::new_v4().to_string();
        let _ = state.db.add_task_message(&assistant_msg_id, &request.task_id, "assistant", &forced.final_text);
        let _ = state.db.update_task_status(&request.task_id, "completed");
        let _ = window.emit("agent-event", AgentEvent::Text { content: forced.final_text });
        let _ = window.emit("agent-event", AgentEvent::Done { total_turns: 1 });
        return Ok("Task completed successfully".to_string());
    }

    // Build agent config with MCP servers info
    let mut config = AgentConfig::default();

    // Add MCP servers info to system prompt
    let mcp_servers = state.mcp_manager.get_server_statuses().await;
    let mut mcp_info = String::new();
    if !mcp_servers.is_empty() {
        mcp_info.push_str("\nMCP (Model Context Protocol) Tools:\n");
        for server in mcp_servers {
            if matches!(server.status, crate::mcp::types::ConnectionStatus::Connected) {
                mcp_info.push_str(&format!("Server '{}' is connected with tools:\n", server.id));
                for tool in server.tools {
                    mcp_info.push_str(&format!("  - {}: {} (use format: {}:{})\n",
                        tool.name, tool.description, server.id, tool.name));
                }
            }
        }
    }
    if !mcp_info.is_empty() {
        config.system_prompt.push_str(&mcp_info);
    }

    if let Some(turns) = request.max_turns {
        config.max_turns = turns;
    }
    config.project_path = effective_project_path.clone();
    if let Some(project_path) = &effective_project_path {
        config.system_prompt.push_str(&format!(
            "\n\n## Workspace Constraints\nMounted folder(s): {}\nAlways read and write files only inside mounted folder(s). Avoid temporary directories unless user explicitly asks.",
            project_path
        ));
    }

    // Get provider info
    let provider_id = settings.get_provider();

    // Create agent loop with provider
    let agent = AgentLoop::new_with_provider(
        settings.api_key,
        settings.base_url,
        config,
        settings.model,
        settings.max_tokens,
        Some(settings.temperature),
        state.mcp_manager.clone(),
        Some(&provider_id),
    );

    // Build conversation history from existing messages
    let mut agent_messages: Vec<AgentMessage> = existing_messages
        .iter()
        .map(|m| AgentMessage {
            role: m.role.clone(),
            content: AgentContent::Text(m.content.clone()),
        })
        .collect();

    // Add the new user message
    agent_messages.push(AgentMessage {
        role: "user".to_string(),
        content: build_user_content_with_images(
            &request.message,
            request.image_paths.as_deref().unwrap_or(&[]),
            request.image_data.as_deref().unwrap_or(&[]),
            effective_project_path.as_deref(),
        ),
    });

    // Create channel for events
    let (tx, mut rx) = tokio::sync::mpsc::channel::<AgentEvent>(100);

    // Clone state for event handler
    let task_id = request.task_id.clone();
    let task_id_for_msg = request.task_id.clone();
    let db = state.db.clone();
    let db_for_msg = state.db.clone();

    // Track accumulated text for saving
    let accumulated_text = std::sync::Arc::new(std::sync::Mutex::new(String::new()));
    let accumulated_text_clone = accumulated_text.clone();
    let last_tool_output = std::sync::Arc::new(std::sync::Mutex::new(None::<String>));
    let last_tool_output_clone = last_tool_output.clone();
    let tool_call_count = std::sync::Arc::new(std::sync::Mutex::new(0usize));
    let tool_call_count_clone = tool_call_count.clone();

    // Spawn event emitter with task tracking
    let window_clone = window.clone();
    let emit_task = tokio::spawn(async move {
        while let Some(event) = rx.recv().await {
            // Track plan and step updates in database
            match &event {
                AgentEvent::Text { content } => {
                    // Update accumulated text
                    if let Ok(mut text) = accumulated_text_clone.lock() {
                        *text = content.clone();
                    }
                }
                AgentEvent::Plan { steps } => {
                    let plan_steps: Vec<PlanStep> = steps.iter().map(|s| PlanStep {
                        step: s.step,
                        description: s.description.clone(),
                        status: "pending".to_string(),
                    }).collect();
                    let _ = db.update_task_plan(&task_id, &plan_steps);
                }
                AgentEvent::StepStart { step } => {
                    let _ = db.update_task_step(&task_id, *step, "running");
                }
                AgentEvent::StepDone { step } => {
                    let _ = db.update_task_step(&task_id, *step, "completed");
                }
                AgentEvent::ToolEnd { result, .. } => {
                    if let Ok(mut count) = tool_call_count_clone.lock() {
                        *count += 1;
                    }
                    if !result.trim().is_empty() {
                        let mut summary = result.trim().to_string();
                        if summary.chars().count() > 1200 {
                            summary = summary.chars().take(1200).collect::<String>() + "...";
                        }
                        if let Ok(mut last) = last_tool_output_clone.lock() {
                            *last = Some(summary);
                        }
                    }
                }
                AgentEvent::Done { .. } => {
                    let _ = db.update_task_status(&task_id, "completed");
                }
                AgentEvent::Error { .. } => {
                    let _ = db.update_task_status(&task_id, "failed");
                }
                _ => {}
            }

            // Emit to frontend
            let _ = window_clone.emit("agent-event", &event);
        }
    });

    // Run agent with conversation history
    let result = agent.run_with_history(agent_messages, tx).await;

    // Wait for emitter to finish
    let _ = emit_task.await;

    // Save assistant message with accumulated text
    let final_text = accumulated_text.lock().map(|t| t.clone()).unwrap_or_default();
    let last_tool_output_text = last_tool_output.lock().ok().and_then(|v| v.clone());
    let total_tool_calls = tool_call_count.lock().map(|c| *c).unwrap_or(0);
    let resolved_final_text = if final_text.trim().is_empty() {
        if let Some(tool_output) = last_tool_output_text {
            format!(
                "I completed the tool execution but did not receive a final text response from the model.\n\nTool result summary:\n{}",
                tool_output
            )
        } else if total_tool_calls > 0 {
            format!(
                "I executed {} tool call(s), but the model returned an empty final response.",
                total_tool_calls
            )
        } else {
            "The model returned an empty response for this task.".to_string()
        }
    } else {
        final_text
    };

    let assistant_msg_id = uuid::Uuid::new_v4().to_string();
    let _ = db_for_msg.add_task_message(&assistant_msg_id, &task_id_for_msg, "assistant", &resolved_final_text);

    // Always ensure task status is updated at the end
    match result {
        Ok(_messages) => {
            // Explicitly update to completed (in case event was missed)
            let _ = state.db.update_task_status(&request.task_id, "completed");
            Ok("Task completed successfully".to_string())
        }
        Err(e) => {
            state.db.update_task_status(&request.task_id, "failed")?;
            Err(CommandError { message: e })
        }
    }
}

// Get task messages command
#[command]
pub fn get_task_messages(
    state: State<'_, Arc<AppState>>,
    task_id: String,
) -> Result<Vec<TaskMessage>, CommandError> {
    state.db.get_task_messages(&task_id).map_err(Into::into)
}

// Skills commands
#[command]
pub fn get_skills_list() -> Vec<SkillMetadata> {
    get_available_skills()
}

// MCP commands
#[command]
pub fn list_mcp_servers(state: State<'_, Arc<AppState>>) -> Result<Vec<MCPServerConfig>, CommandError> {
    state.db.get_mcp_servers().map_err(|e| CommandError {
        message: format!("Failed to get MCP servers: {}", e)
    })
}

#[command]
pub async fn save_mcp_server(
    state: State<'_, Arc<AppState>>,
    config: MCPServerConfig,
) -> Result<(), CommandError> {
    state.db.save_mcp_server(&config).map_err(|e| CommandError {
        message: format!("Failed to save MCP server: {}", e)
    })?;

    // Auto-restart connected server to apply updated config immediately.
    let should_restart = state
        .mcp_manager
        .get_server_statuses()
        .await
        .into_iter()
        .any(|status| {
            status.id == config.id
                && matches!(
                    status.status,
                    crate::mcp::types::ConnectionStatus::Connected
                        | crate::mcp::types::ConnectionStatus::Connecting
                )
        });

    if should_restart {
        state.mcp_manager.disconnect_server(&config.id).await;
        if config.enabled {
            state
                .mcp_manager
                .connect_server(&config)
                .await
                .map_err(|e| CommandError {
                    message: format!("Server saved, but reconnect failed: {}", e),
                })?;
        }
    }

    Ok(())
}

#[command]
pub async fn test_mcp_server_config(
    state: State<'_, Arc<AppState>>,
    mut config: MCPServerConfig,
) -> Result<MCPServerStatus, CommandError> {
    let test_id = format!("test-{}", uuid::Uuid::new_v4());
    config.id = test_id.clone();
    config.enabled = true;

    state
        .mcp_manager
        .connect_server(&config)
        .await
        .map_err(|e| CommandError {
            message: format!("MCP config test failed: {}", e),
        })?;

    let status = state
        .mcp_manager
        .get_server_statuses()
        .await
        .into_iter()
        .find(|s| s.id == test_id)
        .unwrap_or(MCPServerStatus {
            id: test_id.clone(),
            name: config.name.clone(),
            transport: config.transport.clone(),
            status: crate::mcp::types::ConnectionStatus::Disconnected,
            tools: vec![],
            last_error: Some("No status returned for test connection".to_string()),
            managed_process: false,
            pid: None,
            endpoint: None,
        });

    state.mcp_manager.disconnect_server(&test_id).await;
    Ok(status)
}

#[command]
pub fn delete_mcp_server(
    state: State<'_, Arc<AppState>>,
    id: String,
) -> Result<(), CommandError> {
    state.db.delete_mcp_server(&id).map_err(|e| CommandError {
        message: format!("Failed to delete MCP server: {}", e)
    })
}

#[command]
pub async fn connect_mcp_server(
    state: State<'_, Arc<AppState>>,
    id: String,
) -> Result<(), CommandError> {
    // Get server config from database
    let config = match state.db.get_mcp_server(&id).map_err(|e| CommandError {
        message: format!("Failed to get server config: {}", e)
    })? {
        Some(config) => config,
        None => return Err(CommandError {
            message: "MCP server not found".to_string()
        }),
    };

    // Connect using MCP manager
    state.mcp_manager.connect_server(&config).await.map_err(|e| CommandError {
        message: format!("Failed to connect to MCP server: {}", e)
    })?;

    // Update enabled status in database
    state.db.update_mcp_server_enabled(&id, true).map_err(|e| CommandError {
        message: format!("Failed to update server status: {}", e)
    })
}

#[command]
pub async fn disconnect_mcp_server(
    state: State<'_, Arc<AppState>>,
    id: String,
) -> Result<(), CommandError> {
    // Disconnect using MCP manager
    state.mcp_manager.disconnect_server(&id).await;

    // Update enabled status in database
    state.db.update_mcp_server_enabled(&id, false).map_err(|e| CommandError {
        message: format!("Failed to update server status: {}", e)
    })
}

#[command]
pub async fn get_mcp_server_statuses(
    state: State<'_, Arc<AppState>>,
) -> Result<Vec<MCPServerStatus>, CommandError> {
    Ok(state.mcp_manager.get_server_statuses().await)
}

#[command]
pub async fn execute_mcp_tool(
    state: State<'_, Arc<AppState>>,
    call: MCPToolCall,
) -> Result<MCPToolResult, CommandError> {
    Ok(state.mcp_manager.execute_tool(&call).await)
}

fn should_force_directory_listing_query(message: &str) -> bool {
    let normalized = message.to_lowercase();
    let targets = ["folders", "folder", "files", "file", "directories", "directory", "contents"];
    let has_target = targets.iter().any(|t| normalized.contains(t));
    if !has_target {
        return false;
    }

    // Always force a tool call for file/folder listing-style intents.
    let intent_terms = [
        "list",
        "show",
        "display",
        "view",
        "what are",
        "which",
        "inside",
        "in now",
        "available",
    ];
    intent_terms.iter().any(|t| normalized.contains(t))
}

fn should_force_xlsx_creation_query(message: &str) -> bool {
    let normalized = message.to_lowercase();
    let excel_terms = ["excel", "xlsx", "spreadsheet", "workbook"];
    let action_terms = ["create", "make", "generate", "build"];
    excel_terms.iter().any(|t| normalized.contains(t))
        && action_terms.iter().any(|t| normalized.contains(t))
}

fn should_force_advanced_xlsx_mode(message: &str) -> bool {
    let normalized = message.to_lowercase();
    let advanced_terms = [
        "sheet",
        "sheets",
        "formula",
        "formulas",
        "freeze",
        "filter",
        "column width",
        "row height",
        "summary",
        "inventory",
        "sales",
        "formatting",
        "professional",
    ];
    advanced_terms.iter().any(|t| normalized.contains(t))
}

fn first_workspace_root(project_path: Option<&str>) -> Option<String> {
    project_path
        .unwrap_or("")
        .split(',')
        .map(|p| p.trim())
        .find(|p| !p.is_empty())
        .map(|p| normalize_workspace_output_root(p))
}

fn normalize_workspace_output_root(base: &str) -> String {
    let mut path = PathBuf::from(base);
    if path
        .file_name()
        .and_then(|n| n.to_str())
        .map(|n| n.eq_ignore_ascii_case("src-tauri"))
        .unwrap_or(false)
    {
        if let Some(parent) = path.parent() {
            path = parent.to_path_buf();
        }
    }
    path.to_string_lossy().to_string()
}

fn safe_xlsx_target_path(
    requested_path: Option<String>,
    project_path: Option<&str>,
) -> Result<String, String> {
    let base = first_workspace_root(project_path)
        .or_else(default_workspace_root)
        .ok_or_else(|| "No workspace root available".to_string())?;
    let base_path = PathBuf::from(base);

    let candidate = match requested_path {
        Some(raw) => {
            let p = PathBuf::from(raw);
            if p.is_absolute() {
                if p.starts_with(&base_path) {
                    p
                } else {
                    base_path.join(
                        p.file_name()
                            .ok_or_else(|| "Invalid XLSX path".to_string())?,
                    )
                }
            } else {
                base_path.join(p)
            }
        }
        None => base_path.join("data.xlsx"),
    };

    let file_name = candidate
        .file_name()
        .and_then(|n| n.to_str())
        .unwrap_or("data.xlsx");
    let final_name = if file_name.to_lowercase().ends_with(".xlsx") {
        file_name.to_string()
    } else {
        format!("{}.xlsx", file_name)
    };

    let mut out = candidate;
    out.set_file_name(final_name);
    Ok(out.to_string_lossy().to_string())
}

fn build_advanced_sales_workbook_input(target_path: &str) -> serde_json::Value {
    serde_json::json!({
        "path": target_path,
        "workbook": {
            "sheets": [
                {
                    "name": "Sales",
                    "headers": ["Date", "Region", "Product", "Quantity", "Unit Price", "Line Total"],
                    "column_widths": [14, 14, 18, 12, 12, 14],
                    "row_heights": [22],
                    "freeze_panes": { "row": 1, "col": 0 },
                    "autofilter": { "from_row": 0, "from_col": 0, "to_row": 20, "to_col": 5 },
                    "rows": [
                        ["2026-01-01", "North", "Laptop", 2, 1200, {"formula":"D2*E2"}],
                        ["2026-01-02", "South", "Monitor", 5, 250, {"formula":"D3*E3"}],
                        ["2026-01-03", "East", "Keyboard", 12, 40, {"formula":"D4*E4"}],
                        ["2026-01-04", "West", "Mouse", 10, 25, {"formula":"D5*E5"}],
                        ["2026-01-05", "North", "Dock", 6, 90, {"formula":"D6*E6"}],
                        ["2026-01-06", "South", "Headset", 8, 75, {"formula":"D7*E7"}],
                        ["2026-01-07", "East", "Webcam", 4, 130, {"formula":"D8*E8"}],
                        ["2026-01-08", "West", "Chair", 3, 320, {"formula":"D9*E9"}],
                        ["2026-01-09", "North", "Desk", 2, 480, {"formula":"D10*E10"}],
                        ["2026-01-10", "South", "Laptop", 1, 1200, {"formula":"D11*E11"}],
                        ["2026-01-11", "East", "Monitor", 7, 250, {"formula":"D12*E12"}],
                        ["2026-01-12", "West", "Keyboard", 9, 40, {"formula":"D13*E13"}],
                        ["2026-01-13", "North", "Mouse", 11, 25, {"formula":"D14*E14"}],
                        ["2026-01-14", "South", "Dock", 5, 90, {"formula":"D15*E15"}],
                        ["2026-01-15", "East", "Headset", 6, 75, {"formula":"D16*E16"}],
                        ["2026-01-16", "West", "Webcam", 3, 130, {"formula":"D17*E17"}],
                        ["2026-01-17", "North", "Chair", 2, 320, {"formula":"D18*E18"}],
                        ["2026-01-18", "South", "Desk", 1, 480, {"formula":"D19*E19"}],
                        ["2026-01-19", "East", "Laptop", 2, 1200, {"formula":"D20*E20"}],
                        ["2026-01-20", "West", "Monitor", 4, 250, {"formula":"D21*E21"}]
                    ]
                },
                {
                    "name": "Summary",
                    "headers": ["Metric", "Value"],
                    "column_widths": [28, 18],
                    "rows": [
                        ["Total Revenue", {"formula":"SUM(Sales!F2:F21)"}],
                        ["Average Order Value", {"formula":"AVERAGE(Sales!F2:F21)"}],
                        ["Top Region (by rows)", {"formula":"INDEX({\"North\",\"South\",\"East\",\"West\"},MATCH(MAX(COUNTIF(Sales!B2:B21,{\"North\",\"South\",\"East\",\"West\"})),COUNTIF(Sales!B2:B21,{\"North\",\"South\",\"East\",\"West\"}),0))"}],
                        ["Top Product (sample)", {"formula":"INDEX(Sales!C2:C21,MATCH(MAX(Sales!F2:F21),Sales!F2:F21,0))"}]
                    ]
                },
                {
                    "name": "Inventory",
                    "headers": ["Item", "SKU", "Stock On Hand", "Reorder Level", "Unit Cost", "Stock Value", "Low Stock"],
                    "column_widths": [18, 12, 14, 14, 12, 14, 12],
                    "freeze_panes": { "row": 1, "col": 0 },
                    "autofilter": { "from_row": 0, "from_col": 0, "to_row": 12, "to_col": 6 },
                    "rows": [
                        ["Laptop", "SKU-1001", 12, 8, 900, {"formula":"C2*E2"}, {"formula":"IF(C2<=D2,\"YES\",\"NO\")"}],
                        ["Monitor", "SKU-1002", 30, 12, 170, {"formula":"C3*E3"}, {"formula":"IF(C3<=D3,\"YES\",\"NO\")"}],
                        ["Keyboard", "SKU-1003", 55, 20, 22, {"formula":"C4*E4"}, {"formula":"IF(C4<=D4,\"YES\",\"NO\")"}],
                        ["Mouse", "SKU-1004", 40, 25, 15, {"formula":"C5*E5"}, {"formula":"IF(C5<=D5,\"YES\",\"NO\")"}],
                        ["Dock", "SKU-1005", 9, 10, 60, {"formula":"C6*E6"}, {"formula":"IF(C6<=D6,\"YES\",\"NO\")"}],
                        ["Headset", "SKU-1006", 14, 10, 40, {"formula":"C7*E7"}, {"formula":"IF(C7<=D7,\"YES\",\"NO\")"}],
                        ["Webcam", "SKU-1007", 6, 8, 75, {"formula":"C8*E8"}, {"formula":"IF(C8<=D8,\"YES\",\"NO\")"}],
                        ["Chair", "SKU-1008", 5, 6, 190, {"formula":"C9*E9"}, {"formula":"IF(C9<=D9,\"YES\",\"NO\")"}],
                        ["Desk", "SKU-1009", 3, 4, 260, {"formula":"C10*E10"}, {"formula":"IF(C10<=D10,\"YES\",\"NO\")"}],
                        ["UPS", "SKU-1010", 7, 5, 120, {"formula":"C11*E11"}, {"formula":"IF(C11<=D11,\"YES\",\"NO\")"}],
                        ["Cable Kit", "SKU-1011", 80, 30, 8, {"formula":"C12*E12"}, {"formula":"IF(C12<=D12,\"YES\",\"NO\")"}],
                        ["Adapter", "SKU-1012", 24, 15, 18, {"formula":"C13*E13"}, {"formula":"IF(C13<=D13,\"YES\",\"NO\")"}]
                    ]
                }
            ]
        }
    })
}

fn build_simple_xlsx_input(target_path: &str) -> serde_json::Value {
    serde_json::json!({
        "path": target_path,
        "sheet_name": "Sheet1",
        "rows": [[""]],
        "strict": true
    })
}

fn try_force_xlsx_creation(message: &str, project_path: Option<&str>) -> Option<ForcedExecution> {
    if !should_force_xlsx_creation_query(message) {
        return None;
    }

    let message_path_re = Regex::new(r#"([A-Za-z]:\\[^\s"'`]+\.xlsx|[^\s"'`]+\.xlsx)"#).ok()?;
    let requested_path = message_path_re
        .find(message)
        .map(|m| m.as_str().to_string());

    let target_path = match safe_xlsx_target_path(requested_path, project_path) {
        Ok(path) => path,
        Err(err) => {
            return Some(ForcedExecution {
                final_text: format!("Unable to choose a safe XLSX output path: {}", err),
                previews: vec![],
            })
        }
    };

    let wants_advanced = should_force_advanced_xlsx_mode(message);
    let input = if wants_advanced {
        let normalized = message.to_lowercase();
        let has_sales_summary_inventory = ["sales", "summary", "inventory"]
            .iter()
            .all(|k| normalized.contains(k));
        if !has_sales_summary_inventory {
            return Some(ForcedExecution {
                final_text: "I can create a complex workbook, but I need one detail: provide target sheet names (comma-separated) so I can build and validate it strictly.".to_string(),
                previews: vec![],
            });
        }
        build_advanced_sales_workbook_input(&target_path)
    } else {
        build_simple_xlsx_input(&target_path)
    };

    match crate::tools::xlsx_create::execute(&input, project_path) {
        Ok(msg) => Some(ForcedExecution {
            final_text: format!("Created Excel file successfully with strict validation.\n{}", msg),
            previews: vec![ForcedToolPreview {
                tool: "create_xlsx_file (forced)".to_string(),
                input,
                result: msg,
                success: true,
            }],
        }),
        Err(err) => Some(ForcedExecution {
            final_text: format!("Failed to create Excel file: {}", err),
            previews: vec![ForcedToolPreview {
                tool: "create_xlsx_file (forced)".to_string(),
                input,
                result: err,
                success: false,
            }],
        }),
    }
}

async fn try_force_directory_listing(
    mcp_manager: &MCPManager,
    message: &str,
) -> Option<ForcedExecution> {
    if !should_force_directory_listing_query(message) {
        return None;
    }

    let connected = mcp_manager
        .get_server_statuses()
        .await
        .into_iter()
        .find(|s| {
            matches!(s.status, crate::mcp::types::ConnectionStatus::Connected)
                && s.tools.iter().any(|t| t.name == "list_directory")
                && s.tools.iter().any(|t| t.name == "list_allowed_directories")
        })?;

    let allowed = mcp_manager
        .execute_tool(&MCPToolCall {
            server_id: connected.id.clone(),
            tool_name: "list_allowed_directories".to_string(),
            parameters: serde_json::json!({}),
        })
        .await;

    if !allowed.success {
        let error_text = format!(
            "I attempted to list directories using MCP, but failed to read allowed directories: {}",
            allowed.error.unwrap_or_else(|| "unknown error".to_string())
        );
        return Some(ForcedExecution {
            final_text: error_text.clone(),
            previews: vec![ForcedToolPreview {
                tool: "list_allowed_directories (forced)".to_string(),
                input: serde_json::json!({}),
                result: error_text,
                success: false,
            }],
        });
    }

    let allowed_text = extract_mcp_result_text(&allowed.result);
    let root_path = extract_first_windows_path(&allowed_text).unwrap_or_else(|| ".".to_string());

    let listed = mcp_manager
        .execute_tool(&MCPToolCall {
            server_id: connected.id.clone(),
            tool_name: "list_directory".to_string(),
            parameters: serde_json::json!({ "path": root_path }),
        })
        .await;

    if !listed.success {
        let error_text = format!(
            "Allowed directories:\n{}\n\nI attempted to list folders, but the tool call failed: {}",
            allowed_text,
            listed.error.unwrap_or_else(|| "unknown error".to_string())
        );
        return Some(ForcedExecution {
            final_text: error_text.clone(),
            previews: vec![
                ForcedToolPreview {
                    tool: "list_allowed_directories (forced)".to_string(),
                    input: serde_json::json!({}),
                    result: allowed_text,
                    success: true,
                },
                ForcedToolPreview {
                    tool: "list_directory (forced)".to_string(),
                    input: serde_json::json!({ "path": root_path }),
                    result: error_text,
                    success: false,
                },
            ],
        });
    }

    let listing_text = extract_mcp_result_text(&listed.result);
    Some(ForcedExecution {
        final_text: format!(
            "Allowed directories:\n{}\n\nDirectory listing:\n{}",
            allowed_text, listing_text
        ),
        previews: vec![
            ForcedToolPreview {
                tool: "list_allowed_directories (forced)".to_string(),
                input: serde_json::json!({}),
                result: allowed_text,
                success: true,
            },
            ForcedToolPreview {
                tool: "list_directory (forced)".to_string(),
                input: serde_json::json!({ "path": root_path }),
                result: listing_text,
                success: true,
            },
        ],
    })
}

fn extract_mcp_result_text(value: &serde_json::Value) -> String {
    if let Some(content_arr) = value.get("content").and_then(|v| v.as_array()) {
        let mut out = Vec::new();
        for item in content_arr {
            if let Some(text) = item.get("text").and_then(|v| v.as_str()) {
                out.push(text.to_string());
            }
        }
        if !out.is_empty() {
            return out.join("\n");
        }
    }

    if let Some(text) = value
        .get("structuredContent")
        .and_then(|v| v.get("content"))
        .and_then(|v| v.as_str())
    {
        return text.to_string();
    }

    value.to_string()
}

fn extract_first_windows_path(input: &str) -> Option<String> {
    let re = Regex::new(r"[A-Za-z]:\\[^,\r\n]+").ok()?;
    re.find(input).map(|m| m.as_str().trim().to_string())
}

fn build_user_content_with_images(
    message: &str,
    image_paths: &[String],
    image_data: &[ImageAttachmentInput],
    project_path: Option<&str>,
) -> AgentContent {
    if image_paths.is_empty() && image_data.is_empty() {
        return AgentContent::Text(message.to_string());
    }

    let mut blocks = vec![ContentBlock::Text {
        text: message.to_string(),
    }];

    for image_path in image_paths {
        let resolved = match crate::tools::path_utils::resolve_path_for_write(
            std::path::Path::new(image_path),
            project_path,
        ) {
            Ok(p) => p,
            Err(_) => continue,
        };

        let ext = resolved
            .extension()
            .and_then(|e| e.to_str())
            .unwrap_or("")
            .to_lowercase();

        let media_type = match ext.as_str() {
            "png" => "image/png",
            "jpg" | "jpeg" => "image/jpeg",
            "webp" => "image/webp",
            "gif" => "image/gif",
            _ => continue,
        };

        let bytes = match fs::read(&resolved) {
            Ok(b) => b,
            Err(_) => continue,
        };

        // Keep request sizes manageable and avoid provider rejections.
        if bytes.len() > 10 * 1024 * 1024 {
            continue;
        }

        let encoded = general_purpose::STANDARD.encode(bytes);
        blocks.push(ContentBlock::Image {
            source: ImageSource {
                source_type: "base64".to_string(),
                media_type: media_type.to_string(),
                data: encoded,
            },
        });
    }

    for inline in image_data {
        let media = inline.media_type.to_lowercase();
        let allowed = [
            "image/png",
            "image/jpeg",
            "image/webp",
            "image/gif",
        ];
        if !allowed.contains(&media.as_str()) {
            continue;
        }
        if inline.data.len() > 14 * 1024 * 1024 {
            continue;
        }
        blocks.push(ContentBlock::Image {
            source: ImageSource {
                source_type: "base64".to_string(),
                media_type: media,
                data: inline.data.clone(),
            },
        });
    }

    AgentContent::Blocks(blocks)
}

/// Convert Claude API request format to OpenAI format
fn convert_to_openai_format(
    request: &crate::agent::message_builder::ClaudeApiRequest,
    model: &str,
) -> serde_json::Value {
    use crate::agent::message_builder::ApiContent;

    // Build messages, including system prompt
    let mut messages: Vec<serde_json::Value> = Vec::new();

    // Add system message
    if !request.system.is_empty() {
        messages.push(serde_json::json!({
            "role": "system",
            "content": request.system
        }));
    }

    // Convert conversation messages
    for msg in &request.messages {
        let role = &msg.role;

        match &msg.content {
            ApiContent::Text(text) => {
                messages.push(serde_json::json!({
                    "role": role,
                    "content": text
                }));
            }
            ApiContent::Blocks(blocks) => {
                // Handle content blocks (text, tool_use, tool_result)
                let mut text_parts: Vec<String> = Vec::new();
                let mut tool_calls: Vec<serde_json::Value> = Vec::new();
                let mut content_parts: Vec<serde_json::Value> = Vec::new();
                let mut has_image = false;

                for block in blocks {
                    let block_type = block.get("type").and_then(|v| v.as_str()).unwrap_or("");

                    match block_type {
                        "text" => {
                            if let Some(text) = block.get("text").and_then(|v| v.as_str()) {
                                text_parts.push(text.to_string());
                                content_parts.push(serde_json::json!({
                                    "type": "text",
                                    "text": text
                                }));
                            }
                        }
                        "image" => {
                            let media = block
                                .get("source")
                                .and_then(|v| v.get("media_type"))
                                .and_then(|v| v.as_str());
                            let data = block
                                .get("source")
                                .and_then(|v| v.get("data"))
                                .and_then(|v| v.as_str());

                            if let (Some(media_type), Some(base64_data)) = (media, data) {
                                has_image = true;
                                content_parts.push(serde_json::json!({
                                    "type": "image_url",
                                    "image_url": {
                                        "url": format!("data:{};base64,{}", media_type, base64_data)
                                    }
                                }));
                            }
                        }
                        "tool_use" => {
                            tool_calls.push(serde_json::json!({
                                "id": block.get("id"),
                                "type": "function",
                                "function": {
                                    "name": block.get("name"),
                                    "arguments": serde_json::to_string(block.get("input").unwrap_or(&serde_json::json!({}))).unwrap_or_default()
                                }
                            }));
                        }
                        "tool_result" => {
                            // OpenAI uses tool role to represent tool results
                            messages.push(serde_json::json!({
                                "role": "tool",
                                "tool_call_id": block.get("tool_use_id"),
                                "content": block.get("content")
                            }));
                        }
                        _ => {}
                    }
                }

                // If there's text content
                if !text_parts.is_empty() {
                    let mut msg_obj = serde_json::json!({
                        "role": role,
                        "content": if has_image {
                            serde_json::Value::Array(content_parts.clone())
                        } else {
                            serde_json::Value::String(text_parts.join("\n"))
                        }
                    });

                    // If there are tool_calls
                    if !tool_calls.is_empty() {
                        msg_obj["tool_calls"] = serde_json::json!(tool_calls);
                    }

                    messages.push(msg_obj);
                } else if !content_parts.is_empty() {
                    messages.push(serde_json::json!({
                        "role": role,
                        "content": content_parts
                    }));
                } else if !tool_calls.is_empty() {
                    // Only tool_calls, no text
                    messages.push(serde_json::json!({
                        "role": role,
                        "content": serde_json::Value::Null,
                        "tool_calls": tool_calls
                    }));
                }
            }
        }
    }

    // Convert tools definition
    let tools: Vec<serde_json::Value> = request.tools.iter().map(|tool| {
        serde_json::json!({
            "type": "function",
            "function": {
                "name": tool.name,
                "description": tool.description,
                "parameters": tool.input_schema
            }
        })
    }).collect();

    let mut openai_request = serde_json::json!({
        "model": request.model,
        "stream": request.stream,
        "messages": messages
    });

    // Use correct max tokens parameter based on model
    let model_lower = model.to_lowercase();
    let is_legacy = model_lower.contains("gpt-3.5")
        || (model_lower.contains("gpt-4") && !model_lower.contains("gpt-4o") && !model_lower.contains("gpt-4-turbo"));

    if is_legacy {
        openai_request["max_tokens"] = serde_json::json!(request.max_tokens);
    } else {
        openai_request["max_completion_tokens"] = serde_json::json!(request.max_tokens);
    }

    // Only add temperature for non-reasoning models (o1, o3, gpt-5 don't support custom temperature)
    let is_reasoning = model_lower.starts_with("o1") || model_lower.starts_with("o3") || model_lower.starts_with("gpt-5")
        || model_lower.contains("-o1") || model_lower.contains("-o3")
        || model_lower.contains("o1-") || model_lower.contains("o3-");

    if !is_reasoning {
        if let Some(temp) = request.temperature {
            openai_request["temperature"] = serde_json::json!(temp);
        }
    }

    if !tools.is_empty() {
        openai_request["tools"] = serde_json::json!(tools);
        openai_request["tool_choice"] = serde_json::json!("auto");
    }

    openai_request
}

/// Convert Claude API request format to Google Gemini format
fn convert_to_google_format(
    request: &crate::agent::message_builder::ClaudeApiRequest,
    _model: &str,
    max_tokens: u32,
    thought_signatures: &std::collections::HashMap<String, String>,
) -> serde_json::Value {
    use crate::agent::message_builder::ApiContent;

    // Build contents array
    let mut contents: Vec<serde_json::Value> = Vec::new();

    // Convert messages to Google format
    for msg in &request.messages {
        // Google uses "user" and "model" instead of "user" and "assistant"
        let role = if msg.role == "assistant" { "model" } else { &msg.role };

        let parts = match &msg.content {
            ApiContent::Text(text) => {
                vec![serde_json::json!({"text": text})]
            }
            ApiContent::Blocks(blocks) => {
                let mut parts_list = Vec::new();
                for block in blocks {
                    let block_type = block.get("type").and_then(|v| v.as_str()).unwrap_or("");

                    match block_type {
                        "text" => {
                            if let Some(text) = block.get("text").and_then(|v| v.as_str()) {
                                parts_list.push(serde_json::json!({"text": text}));
                            }
                        }
                        "image" => {
                            let media = block
                                .get("source")
                                .and_then(|v| v.get("media_type"))
                                .and_then(|v| v.as_str());
                            let data = block
                                .get("source")
                                .and_then(|v| v.get("data"))
                                .and_then(|v| v.as_str());
                            if let (Some(media_type), Some(base64_data)) = (media, data) {
                                parts_list.push(serde_json::json!({
                                    "inlineData": {
                                        "mimeType": media_type,
                                        "data": base64_data
                                    }
                                }));
                            }
                        }
                        "tool_use" => {
                            // Convert to functionCall format with thoughtSignature if present (for Gemini 3)
                            let tool_id = block.get("id").and_then(|v| v.as_str()).unwrap_or("");
                            let mut fc_part = serde_json::json!({
                                "functionCall": {
                                    "name": block.get("name"),
                                    "args": block.get("input")
                                }
                            });
                            // Include thoughtSignature if we have it for this tool
                            if let Some(sig) = thought_signatures.get(tool_id) {
                                fc_part["thoughtSignature"] = serde_json::json!(sig);
                            }
                            parts_list.push(fc_part);
                        }
                        "tool_result" => {
                            // Convert to functionResponse format with thoughtSignature (required for Gemini 3)
                            let tool_use_id = block.get("tool_use_id").and_then(|v| v.as_str()).unwrap_or("unknown");
                            let mut fr_part = serde_json::json!({
                                "functionResponse": {
                                    "name": tool_use_id,
                                    "response": {
                                        "content": block.get("content")
                                    }
                                }
                            });
                            // Include thoughtSignature from matching tool_use (required for Gemini 3)
                            if let Some(sig) = thought_signatures.get(tool_use_id) {
                                fr_part["thoughtSignature"] = serde_json::json!(sig);
                            }
                            parts_list.push(fr_part);
                        }
                        _ => {}
                    }
                }
                parts_list
            }
        };

        if !parts.is_empty() {
            contents.push(serde_json::json!({
                "role": role,
                "parts": parts
            }));
        }
    }

    // Convert tools to Google functionDeclarations format
    let function_declarations: Vec<serde_json::Value> = request.tools.iter().map(|tool| {
        serde_json::json!({
            "name": tool.name,
            "description": tool.description,
            "parameters": tool.input_schema
        })
    }).collect();

    let mut google_request = serde_json::json!({
        "contents": contents,
        "generationConfig": {
            "maxOutputTokens": max_tokens
        }
    });

    // Add system instruction if present
    if !request.system.is_empty() {
        google_request["systemInstruction"] = serde_json::json!({
            "parts": [{"text": request.system}]
        });
    }

    // Add tools if present
    if !function_declarations.is_empty() {
        google_request["tools"] = serde_json::json!([{
            "functionDeclarations": function_declarations
        }]);
    }

    google_request
}
